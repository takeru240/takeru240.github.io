\subsubsection{OLS}
Let us begin with the matrix representation of the linear regression model,
\begin{align}
\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}
\end{align}
Where,\\
\(\mathbf{y}\): \(n \times 1\) dependent or outcome variable.\\
\(\mathbf{X}\): \(n \times k\) matrix of independent or explanatory variables (treatment and control variables).\\
\(\boldsymbol{\beta}\): \(k \times 1\) vector of coefficients which we aim to estimate.\\
\(\boldsymbol{\varepsilon}\): \(n \times 1\) vector of error terms or residuals.\\
The goal of the OLS model is to estimate \(\boldsymbol{\beta}\), which tells us how changes in the explanatory variables \(\mathbf{X}\) impact the outcome \(\mathbf{y}\). For causal inference we need to determine whether the estimated relationship is causal or merely correlation. Using matrices, we can construct and manipulate various estimators that can control for confounding variables to determine casual effects. 
\subsection {2SLS and IV}
Two-Stage Least Squares addresses the issue when one or more columns of \(\mathbf{X}\) (as in multiple dependent variables) are endogenous, as in correlated with the error term \(\varepsilon\). 2SLS mitigates this by using instrumental variables (IVs) in a two-stage process: first, it projects the endogenous variables onto a space where they are exogenous using the IVs, and then applies OLS to the transformed variables. The chosen instruments influence the endogenous variables but have no direct effect on the dependent variable, isolating the exogenous variation in the explanatory variables.
\subsubsection{Stage 1: Instrumental Variable and Projection}
Let \(\mathbf{Z}\) be an \(n \times m\) matrix of IVs, where \(m \geq k\). The IVs should be correlated with the variables in \(\mathbf{X}\) that are endogenous, but is uncorrelated with \(\varepsilon\). For the first stage, we project \(\mathbf{X}\) onto the space spanned by \(\mathbf{Z}\) to get the predicted values \(\hat{\mathbf{X}}\),
\begin{align}
\hat{\mathbf{X}} = \mathbf{Z}(\mathbf{Z}^\top \mathbf{Z})^{-1}\mathbf{Z}^\top \mathbf{X}
\end{align}
Where,\\
\(\mathbf{Z}^\top \mathbf{Z}\) is an \(m \times m\) matrix. \\
\((\mathbf{Z}^\top \mathbf{Z})^{-1}\) is the inverse of the \(m \times m\) matrix (assuming that it is invertible). \\
\(\mathbf{Z}(\mathbf{Z}^\top \mathbf{Z})^{-1}\mathbf{Z}^\top\) is an \(n \times n\) projection matrix that maps \(\mathbf{X}\) into the space spanned by \(\mathbf{Z}\).
\subsubsection{Stage 2: Regression on Predicted Values}
Next, we regress \(\mathbf{y}\) on the predicted values \(\hat{\mathbf{X}}\) to obtain the 2SLS estimator for \(\beta\),
\begin{align}
\hat{\beta}_{\text{2SLS}} = (\hat{\mathbf{X}}^\top \hat{\mathbf{X}})^{-1} \hat{\mathbf{X}}^\top \mathbf{y}
\end{align}
Substituting \(\hat{\mathbf{X}} = \mathbf{Z}(\mathbf{Z}^\top \mathbf{Z})^{-1}\mathbf{Z}^\top \mathbf{X}\) from the first stage we get,
\begin{align}
\hat{\beta}_{\text{2SLS}} = \left[\mathbf{X}^\top \mathbf{Z}(\mathbf{Z}^\top \mathbf{Z})^{-1} \mathbf{Z}^\top \mathbf{X}\right]^{-1} \mathbf{X}^\top \mathbf{Z}(\mathbf{Z}^\top \mathbf{Z})^{-1} \mathbf{Z}^\top \mathbf{y}
\end{align}
% \begin{align}
% \hat{\mathbf{\beta}}_{\text{2SLS}} = \left[\left(\mathbf{Z} (\mathbf{Z}^\top \mathbf{Z})^{-1} \mathbf{Z}^\top \mathbf{X}\right)^\top \mathbf{Z} (\mathbf{Z}^\top \mathbf{Z})^{-1} \mathbf{Z}^\top \mathbf{X}\right]^{-1} \left(\mathbf{Z} (\mathbf{Z}^\top \mathbf{Z})^{-1} \mathbf{Z}^\top \mathbf{X}\right)^\top \mathbf{y}
% \end{align}
This gives us the 2SLS estimator which is consistent even with endogeneity, as long as the IVs in \(\mathbf{Z}\) are valid. The projection matrix \(\mathbf{P}_Z = \mathbf{Z}(\mathbf{Z}^\top \mathbf{Z})^{-1}\mathbf{Z}^\top\) from 2SLS is symmetric and idempotent (\(\mathbf{P}_Z^2 = \mathbf{P}_Z\)). For the 2SLS estimator to be identified, \(\mathbf{Z}\) must have full column rank, meaning that \(\mathbf{Z}^\top \mathbf{Z}\) is invertible. Under regularity conditions, \(\hat{\beta}_{\text{2SLS}}\) is asymptotically normally distributed,
\begin{align}
\hat{\beta}_{\text{2SLS}} \sim \mathcal{N}\left(\beta, \sigma^2 (\mathbf{X}^\top \mathbf{P}_Z \mathbf{X})^{-1}\right)
\end{align}
where \(\sigma^2\) is the variance of the error term \(\varepsilon\).
\subsubsection{Example}
Consider a model with a single endogenous regressor \(X_1\) and a single exogenous regressor \(X_2\),
\begin{align}
y = \beta_1 X_1 + \beta_2 X_2 + \varepsilon
\end{align}
Suppose we identified an IV \(Z\) for \(X_1\) where,
\begin{align}
\mathbf{X} = \begin{bmatrix} X_1 & X_2 \end{bmatrix}, \quad \mathbf{Z} = \begin{bmatrix} Z \end{bmatrix}
\end{align}
Then the projection of \(\mathbf{X}\) onto \(\mathbf{Z}\) is,
\begin{align}
\hat{\mathbf{X}} = \mathbf{Z}(\mathbf{Z}^\top \mathbf{Z})^{-1}\mathbf{Z}^\top \mathbf{X}
\end{align}
Thus the 2SLS estimator for \(\beta_1\) and \(\beta_2\) would be,
\begin{align}
\hat{\beta}_{\text{2SLS}} &= \left(\hat{\mathbf{X}}^\top \hat{\mathbf{X}}\right)^{-1} \hat{\mathbf{X}}^\top \mathbf{y} \\
\hat{\beta}_{\text{2SLS}} &= \left[\left(\mathbf{Z}(\mathbf{Z}^\top \mathbf{Z})^{-1} \mathbf{Z}^\top \mathbf{X}\right)^\top \mathbf{Z}(\mathbf{Z}^\top \mathbf{Z})^{-1} \mathbf{Z}^\top \mathbf{X}\right]^{-1} \left(\mathbf{Z}(\mathbf{Z}^\top \mathbf{Z})^{-1} \mathbf{Z}^\top \mathbf{X}\right)^\top \mathbf{y} \\
\hat{\beta}_{\text{2SLS}} &= \left[\mathbf{X}^\top \mathbf{Z} (\mathbf{Z}^\top \mathbf{Z})^{-1} \mathbf{Z}^\top \mathbf{X}\right]^{-1} \mathbf{X}^\top \mathbf{Z} (\mathbf{Z}^\top \mathbf{Z})^{-1} \mathbf{Z}^\top \mathbf{y}
\end{align}
By using IV in this way, 2SLS addresses the endogenity problem and ensures that the estimated coefficients represent causal relationships rather than spurious correlation.
We can further solve the 2SLS estimator using the SVD of the IV. The SVD of the instrument matrix \(\mathbf{Z}\) is,
\begin{align}
\mathbf{Z} = \mathbf{U} \mathbf{D} \mathbf{V}^\top
\end{align}
Where, \\
\(\mathbf{U}\) is an \(n \times n\) orthogonal matrix. \\
\(\mathbf{D}\) is an \(n \times m\) diagonal matrix (with singular values on the diagonal). \\
\(\mathbf{V}\) is an \(m \times m\) orthogonal matrix. \\
Next we substitute the SVD of \(\mathbf{Z}\) into the projection formula,
\begin{align}
\mathbf{Z} (\mathbf{Z}^\top \mathbf{Z})^{-1} \mathbf{Z}^\top = \mathbf{U} \mathbf{D} \mathbf{V}^\top \left(\mathbf{V} \mathbf{D}^\top \mathbf{U}^\top \mathbf{U} \mathbf{D} \mathbf{V}^\top\right)^{-1} \mathbf{U} \mathbf{D} \mathbf{V}^\top
\end{align}
Since \(\mathbf{U}^\top \mathbf{U} = \mathbf{I}\) and \(\mathbf{V}^\top \mathbf{V} = \mathbf{I}\),
\begin{align}
\mathbf{Z} (\mathbf{Z}^\top \mathbf{Z})^{-1} \mathbf{Z}^\top &= \mathbf{U} \mathbf{D} \mathbf{D}^{-2} \mathbf{D}^\top \mathbf{U}^\top \\
&= \mathbf{U} \mathbf{D}^{-1} \mathbf{U}^\top
\end{align}
Substituting the SVD projection back to the 2SLS estimator we have,
\begin{align}
\hat{\beta}_{\text{2SLS}} = \left[\mathbf{X}^\top \mathbf{U} \mathbf{D}^{-1} \mathbf{U}^\top \mathbf{X}\right]^{-1} \mathbf{X}^\top \mathbf{U} \mathbf{D}^{-1} \mathbf{U}^\top \mathbf{y}
\end{align}
SVD is one way to solve for the pseudo-inverse in the case where \(\mathbf{Z}^\top \mathbf{Z}\) may not be invertible directly. In particular, SVD is useful for obtaining the 2SLS estimator when there is strong multicollinearity. When there is multicollinearity, some of the columns of matrix \(\mathbf{X}\) (or \(\mathbf{Z}\) for 2SLS) are nearly linearly dependent. If that is the case then \(\mathbf{X}^\top \mathbf{X}\) or \(\mathbf{Z}^\top \mathbf{Z}\) are nearly singular, as in the determinant is close to 0 and is thus difficult to invert. SVD decomposes the matrix into singular values and vectors which provide a clear indication of the rank deficiency or the extent of multicollinearity. Very small singular values can be treated as zero in a process called regularization which effectively reduces the rank of the matrix and helps stabilize the solution such as is with the process of L2 Regularization, also known as Ridge Regression.